{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b3d469",
   "metadata": {},
   "source": [
    "# Q2: Data Cleaning\n",
    "\n",
    "**Phase 3:** Data Cleaning & Preprocessing  \n",
    "**Points: 9 points**\n",
    "\n",
    "**Focus:** Handle missing data, outliers, validate data types, remove duplicates.\n",
    "\n",
    "**Lecture Reference:** Lecture 11, Notebook 1 ([`11/demo/01_setup_exploration_cleaning.ipynb`](https://github.com/christopherseaman/datasci_217/blob/main/11/demo/01_setup_exploration_cleaning.ipynb)), Phase 3. Also see Lecture 05 (data cleaning).\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a377e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Load data from Q1 (or directly from source)\n",
    "df = pd.read_csv('data/beach_sensors.csv')\n",
    "# If you saved cleaned data from Q1, you can load it:\n",
    "# df = pd.read_csv('output/q1_exploration.csv')  # This won't work - load original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6b0ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Clean the dataset by handling missing data, outliers, validating data types, and removing duplicates.\n",
    "\n",
    "**Time Series Note:** For time series data, forward-fill (`ffill()`) is often appropriate for missing values since sensor readings are continuous. However, you may choose other strategies based on your analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Required Artifacts\n",
    "\n",
    "You must create exactly these 3 files in the `output/` directory:\n",
    "\n",
    "### 1. `output/q2_cleaned_data.csv`\n",
    "**Format:** CSV file\n",
    "**Content:** Cleaned dataset with same structure as original (same columns)\n",
    "**Requirements:**\n",
    "- Same columns as original dataset\n",
    "- Missing values handled (filled, dropped, or imputed)\n",
    "- Outliers handled (removed, capped, or transformed)\n",
    "- Data types validated and converted\n",
    "- Duplicates removed\n",
    "- **Sanity check:** Dataset should retain most rows after cleaning (at least 1,000 rows). If you're removing more than 50% of data, reconsider your strategy—imputation is usually preferable to dropping rows for this dataset.\n",
    "- **No index column** (save with `index=False`)\n",
    "\n",
    "### 2. `output/q2_cleaning_report.txt`\n",
    "**Format:** Plain text file\n",
    "**Content:** Detailed report of cleaning operations\n",
    "**Required information:**\n",
    "- Rows before cleaning: [number]\n",
    "- Missing data handling method: [description]\n",
    "  - Which columns had missing data\n",
    "  - Method used (drop, forward-fill, impute, etc.)\n",
    "  - Number of values handled\n",
    "- Outlier handling: [description]\n",
    "  - Detection method (IQR, z-scores, domain knowledge)\n",
    "  - Which columns had outliers\n",
    "  - Method used (remove, cap, transform)\n",
    "  - Number of outliers handled\n",
    "- Duplicates removed: [number]\n",
    "- Data type conversions: [list any conversions]\n",
    "- Rows after cleaning: [number]\n",
    "\n",
    "**Example format:**\n",
    "```\n",
    "DATA CLEANING REPORT\n",
    "====================\n",
    "\n",
    "Rows before cleaning: 50000\n",
    "\n",
    "Missing Data Handling:\n",
    "- Water Temperature: 2500 missing values (5.0%)\n",
    "  Method: Forward-fill (time series appropriate)\n",
    "  Result: All missing values filled\n",
    "  \n",
    "- Air Temperature: 1500 missing values (3.0%)\n",
    "  Method: Forward-fill, then median imputation for remaining\n",
    "  Result: All missing values filled\n",
    "\n",
    "Outlier Handling:\n",
    "- Water Temperature: Detected 500 outliers using IQR method (3×IQR)\n",
    "  Method: Capped at bounds [Q1 - 3×IQR, Q3 + 3×IQR]\n",
    "  Bounds: [-5.2, 35.8]\n",
    "  Result: 500 values capped\n",
    "\n",
    "Duplicates Removed: 0\n",
    "\n",
    "Data Type Conversions:\n",
    "- Measurement Timestamp: Converted to datetime64[ns]\n",
    "\n",
    "Rows after cleaning: 50000\n",
    "```\n",
    "\n",
    "### 3. `output/q2_rows_cleaned.txt`\n",
    "**Format:** Plain text file\n",
    "**Content:** Single integer number (total rows after cleaning)\n",
    "**Requirements:**\n",
    "- Only the number, no text, no labels\n",
    "- No whitespace before or after\n",
    "- Example: `50000`\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements Checklist\n",
    "\n",
    "- [ ] Missing data handling strategy chosen and implemented\n",
    "- [ ] Outliers detected and handled (IQR method, z-scores, or domain knowledge)\n",
    "- [ ] Data types validated and converted\n",
    "- [ ] Duplicates identified and removed\n",
    "- [ ] Cleaning decisions documented in report\n",
    "- [ ] All 3 required artifacts saved with exact filenames\n",
    "\n",
    "---\n",
    "\n",
    "## Your Approach\n",
    "\n",
    "1. **Handle missing data** - Choose appropriate strategy (drop, forward-fill, impute) based on data characteristics\n",
    "2. **Detect and handle outliers** - Use IQR method or z-scores; decide whether to remove, cap, or transform\n",
    "3. **Validate data types** - Ensure numeric and datetime columns are properly typed\n",
    "4. **Remove duplicates**\n",
    "5. **Document and save** - Write detailed cleaning report explaining your decisions\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Points\n",
    "\n",
    "- **Missing data:** Should you drop rows, impute values, or forward-fill? Consider: How much data is missing? Is it random or systematic? For time series, forward-fill is often appropriate.\n",
    "- **Outliers:** Are they errors or valid extreme values? Use IQR method or z-scores to detect, then decide: remove, cap, or transform. Document your reasoning.\n",
    "- **Data types:** Are numeric columns actually numeric? Are datetime columns properly formatted? Convert as needed.\n",
    "\n",
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "After Q2, you should have:\n",
    "- [ ] Missing data handled\n",
    "- [ ] Outliers addressed\n",
    "- [ ] Data types validated\n",
    "- [ ] Duplicates removed\n",
    "- [ ] All 3 artifacts saved: `q2_cleaned_data.csv`, `q2_cleaning_report.txt`, `q2_rows_cleaned.txt`\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Continue to `q3_data_wrangling.md` for Data Wrangling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7e03aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missing values from previous question\n",
    "\n",
    "#deal with random missing values\n",
    "df['Air Temperature'] = df['Air Temperature'].ffill()\n",
    "df['Barometric Pressure'] = df['Barometric Pressure'].ffill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4148283a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True]\n",
      "Station Name                       0\n",
      "Measurement Timestamp              0\n",
      "Air Temperature                    0\n",
      "Wet Bulb Temperature           75948\n",
      "Humidity                           0\n",
      "Rain Intensity                 75948\n",
      "Interval Rain                      0\n",
      "Total Rain                     75948\n",
      "Precipitation Type             75948\n",
      "Wind Direction                     0\n",
      "Wind Speed                         0\n",
      "Maximum Wind Speed                 0\n",
      "Barometric Pressure                0\n",
      "Solar Radiation                    0\n",
      "Heading                        75948\n",
      "Battery Life                       0\n",
      "Measurement Timestamp Label        0\n",
      "Measurement ID                     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120367"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#deal with patterned missing values\n",
    "\n",
    "# Missing values for Wet Bulp Temp, Total Rain, Precipitation Type, Heading are number of total.\n",
    "# Check to see if these columns are correlated with each other\n",
    "missing_df = df[df['Wet Bulb Temperature'].isnull()]['Station Name'].unique()\n",
    "\n",
    "print(missing_df == df[df['Total Rain'].isnull()]['Station Name'].unique() == df[df['Precipitation Type'].isnull()]['Station Name'].unique() == df[df['Heading'].isnull()]['Station Name'].unique())\n",
    "\n",
    "# We can see that all of the columns are missing values only from a single station.\n",
    "\n",
    "missing_df\n",
    "\n",
    "# This is the Foster Weather Station.\n",
    "# Check missing values for Foster Weather Station\n",
    "\n",
    "foster_df = df[df['Station Name'] == 'Foster Weather Station']\n",
    "\n",
    "print(foster_df.isnull().sum())\n",
    "\n",
    "# Compare to non-misisng values from Foster\n",
    "foster_df.notna().sum()\n",
    "\n",
    "# We can see that there are zero rows with non-missing values for Foster Station.\n",
    "# Because the data is entirely missing, we are not able to fill the values with imputation.\n",
    "# We will drop these rows entirely from the dataset.\n",
    "\n",
    "# We have already imputated random missing values, so we can safely drop the rest of missing values.\n",
    "df_cleaned = df.dropna()\n",
    "len(df_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47abc04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rv/mml0hwl169g87s6l_19swmnw0000gn/T/ipykernel_37175/474528193.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned.ffill(inplace = True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120367"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deal with outliers in all numeric columns using iqr\n",
    "numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# looking at the exploration csv, only some columns have noticeable values that could be outliers. For example,\n",
    "# Solar radiation has a value of -10000000. Because I am not an expert in cutoff values for these categories, \n",
    "# I will use IQR method to identify outliers. In columns that look suspect.\n",
    "\n",
    "numeric_cols_to_check = ['Rain Intensity', 'Interval Rain', 'Barometric Pressure', 'Wind Speed', 'Maximum Wind Speed',\n",
    "                         'Solar Radiation']\n",
    "\n",
    "for col in numeric_cols_to_check:\n",
    "    Q1 = df_cleaned[col].quantile(0.25)\n",
    "    Q3 = df_cleaned[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = (df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)\n",
    "    df_cleaned.loc[outliers, col] = np.nan\n",
    "\n",
    "df_cleaned.ffill(inplace = True)\n",
    "\n",
    "len(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "deefd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates if any\n",
    "pre_dupes = len(df_cleaned)\n",
    "df_cleaned = df_cleaned.drop_duplicates()\n",
    "post_dupes = len(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74d24061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "\n",
    "df_cleaned.to_csv('output/q2_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff216b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Station Name                           object\n",
       "Measurement Timestamp          datetime64[ns]\n",
       "Air Temperature                       float64\n",
       "Wet Bulb Temperature                  float64\n",
       "Humidity                                int64\n",
       "Rain Intensity                        float64\n",
       "Interval Rain                         float64\n",
       "Total Rain                            float64\n",
       "Precipitation Type                    float64\n",
       "Wind Direction                          int64\n",
       "Wind Speed                            float64\n",
       "Maximum Wind Speed                    float64\n",
       "Barometric Pressure                   float64\n",
       "Solar Radiation                       float64\n",
       "Heading                               float64\n",
       "Battery Life                          float64\n",
       "Measurement Timestamp Label    datetime64[ns]\n",
       "Measurement ID                         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.dtypes\n",
    "\n",
    "#change time column to datetime\n",
    "df_cleaned['Measurement Timestamp'] = pd.to_datetime(df_cleaned['Measurement Timestamp'])\n",
    "df_cleaned['Measurement Timestamp Label'] = pd.to_datetime(df_cleaned['Measurement Timestamp Label'])\n",
    "\n",
    "df_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f816ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/q2_cleaning_report.txt', 'w') as f:\n",
    "    f.write(f\"Rows Before Cleaning: {len(df)}\\n\\n\")\n",
    "    f.write(\"Missing Data Description: 7 Columns had missing data. 2 of those columns had random missing data, air temperature and barometric pressure. These were filled using ffill(). The remaining 5 columns, wet bulb temperature, rain intensity, total rain, precipitation type, and heading, had the exact same number of missing data, and all missing values were from Foster Station. Since there was no data from the station at all in these columns, removal was decided rather than attempting to impute data from other stations, as that may not be accurate.\\n\")\n",
    "    f.write(\"\\nOutlier Handling: For outliers, the necessary expertise was lacking to set direct boundaries for each of the categories. However, a quick preliminary search allowed for us to determine reasonable minimum and maximums for each column. From this, we only performed outlier transformation on certain columns. We used IQR method to determine outliers. Outliers were set to NA and then filled using ffill(), assuming that outliers were data entry or measurement errors.\\n\\n\")\n",
    "    f.write(f\"Duplicates Removed: {pre_dupes - post_dupes} \\n\\n\")\n",
    "    f.write(f\"Data Types Cleaned: Changed Measurement Timestamp and Measurement Timestamp Label to datetime64.\\n\\n\")\n",
    "    f.write(f\"Rows After Cleaning: {len(df_cleaned)} \\n\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "env (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
